{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2015)\n",
    "import csv\n",
    "from string import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = open('/Users/junanqu/Desktop/text_p2/train.txt','r').readlines()\n",
    "test_data = open('/Users/junanqu/Desktop/text_p2/test.txt','r').readlines()\n",
    "\n",
    "tags = [\"B-ORG\", \"B-MISC\", \"B-PER\", \"B-LOC\", \"I-ORG\", \"I-MISC\", \"I-PER\", \"I-LOC\", \"O\"]\n",
    "transition_counts = np.zeros((9, 9))\n",
    "ner_dict = {}\n",
    "for k in range(9):\n",
    "    ner_dict[tags[k]] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a customized class to store training-example\n",
    "class integrated_instance():\n",
    "    def __init__(self):\n",
    "        self.sent = \"\"  \n",
    "        self.word = []  \n",
    "        self.pos = [] \n",
    "        self.ner = [] \n",
    "        self.net_test = [] \n",
    "        self.map = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data, type = \"train\"):\n",
    "    if type == \"test\":\n",
    "        sent = \"\"\n",
    "        for i in range(0, len(data), 3):\n",
    "            sent += data[i] + \"\\t\"\n",
    "        pos = \"\"\n",
    "        for i in range(1, len(data), 3):\n",
    "            pos += data[i] + \"\\t\"\n",
    "        ner = \"\"\n",
    "        for i in range(2, len(data), 3):\n",
    "            ner += data[i] + \" \"\n",
    "        \n",
    "        translation = {\"\\r\\n\": None}\n",
    "        \n",
    "        sent = sent.translate(translation)\n",
    "        pos = pos.translate(translation)\n",
    "        ner = ner.translate(translation)\n",
    "\n",
    "        \n",
    "        test_instance = integrated_instance()\n",
    "        test_instance.sent = sent[:-1]\n",
    "        test_instance.word = sent.split(\"\\t\")[:-1]\n",
    "        test_instance.pos = pos.split(\"\\t\")[:-1]\n",
    "        test_instance.ner = ner.split(\" \")[:-1]\n",
    "        return test_instance\n",
    "    \n",
    "    list_sent_instance = []    \n",
    "    \n",
    "    max_idx_line = len(data)\n",
    "    idx_line_loaded = 0\n",
    "    while (idx_line_loaded + 3 <= max_idx_line):\n",
    "        # create a sent_instance object        \n",
    "        new_instance = integrated_instance()\n",
    "        # break strings and         \n",
    "        new_instance.sent = data[idx_line_loaded][:-1] # -2 to get rid of \"\\r\\n\"\n",
    "        new_instance.word = data[idx_line_loaded][:-1].split(\"\\t\")\n",
    "        new_instance.pos = data[idx_line_loaded+1][:-1].split(\"\\t\")\n",
    "        new_instance.ner = data[idx_line_loaded+2][:-1].split(\"\\t\")\n",
    "        \n",
    "        list_sent_instance.append(new_instance)\n",
    "        idx_line_loaded += 3\n",
    "    return list_sent_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = read_data(train_data)\n",
    "test = read_data(test_data, type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emission_prob(sentence_list):\n",
    "    emis_dic={'O':{},'B-LOC':{},'B-ORG':{},'B-PER':{},'B-MISC':{},'I-LOC':{},'I-ORG':{},'I-PER':{},'I-MISC':{}}\n",
    "    for sentence in sentence_list:\n",
    "        for i in range(0, len(sentence.ner),1):\n",
    "            current_word = sentence.word[i]\n",
    "            current_ner = sentence.ner[i]\n",
    "            if (len(current_ner)==0):\n",
    "                continue\n",
    "            if current_word not in emis_dic[current_ner]:\n",
    "                emis_dic[current_ner][current_word]=1.0\n",
    "            else:\n",
    "                emis_dic[current_ner][current_word]+=1.0\n",
    "    \n",
    "    for ner in emis_dic:\n",
    "        num_ner = 0.0\n",
    "        for word in emis_dic[ner]:\n",
    "            num_ner+=emis_dic[ner][word]\n",
    "        for word in emis_dic[ner]:\n",
    "            emis_dic[ner][word]/=num_ner\n",
    "    return emis_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_probability(sentence_list):\n",
    "    num_sentence = len(sentence_list)\n",
    "    start_ner_counts = {}\n",
    "    for sentence in sentence_list:\n",
    "        ner1 = sentence.ner[0]\n",
    "        if ner1 in start_ner_counts:\n",
    "            start_ner_counts[ner1] += 1.0\n",
    "        else:\n",
    "            start_ner_counts[ner1] = 1.0\n",
    "    for ner in start_ner_counts:\n",
    "        start_ner_counts[ner] /= num_sentence\n",
    "    \n",
    "    return start_ner_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_counts(sent_list):\n",
    "    tags = [\"B-ORG\", \"B-MISC\", \"B-PER\", \"B-LOC\", \"I-ORG\", \"I-MISC\", \"I-PER\", \"I-LOC\", \"O\"]\n",
    "    transition_counts = np.zeros((9, 9))\n",
    "    global ner_dict\n",
    "    for k in range(9):\n",
    "        ner_dict[tags[k]] = k\n",
    "    for i in range(0, len(data)):\n",
    "        this_tags = data[i].ner\n",
    "        for j in range(1, len(this_tags)):\n",
    "            if (len(this_tags[j])==0):\n",
    "                continue\n",
    "            op = ner_dict[this_tags[j-1]]\n",
    "            cond = ner_dict[this_tags[j]]\n",
    "            transition_counts[op][cond] = transition_counts[op][cond]+1\n",
    "        \n",
    "    return transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_prob(transition_counts, k):\n",
    "    transition_counts = np.add(transition_counts, k)\n",
    "    transition_probs = np.zeros((9,9))\n",
    "    for i in range(len(transition_counts)): \n",
    "        transition_probs[i] = np.divide(transition_counts[i], np.sum(transition_counts[i]))\n",
    "    return transition_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "def emission_2D_prob(data):\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "\n",
    "    for p in range(len(data)):\n",
    "\n",
    "        this_sentence = data[p].word\n",
    "        this_tags = data[p].ner\n",
    "\n",
    "        if len(this_sentence) == len(this_tags):\n",
    "\n",
    "            for q in range(len(this_sentence)):\n",
    "\n",
    "                all_tokens.append(this_sentence[q])\n",
    "                all_tags.append(this_tags[q])\n",
    "    word_types = np.unique(all_tokens)\n",
    "    \n",
    "    global word_dict\n",
    "    \n",
    "    for t in range(len(word_types)):\n",
    "        word_dict[word_types[t]] = t\n",
    "\n",
    "    lexical_counts = np.zeros((len(word_types)+1, 9))\n",
    "\n",
    "    for u in range(len(all_tokens)):\n",
    "        if (len(all_tags[u])==0):\n",
    "            continue\n",
    "        lexical_counts[word_dict[all_tokens[u]]][ner_dict[all_tags[u]]] = lexical_counts[word_dict[all_tokens[u]]][ner_dict[all_tags[u]]] + 1\n",
    "    (rows, cols) = np.shape(lexical_counts)\n",
    "\n",
    "    for bb in range(cols):\n",
    "\n",
    "        unk_count = 0\n",
    "\n",
    "        this_column = lexical_counts[:,bb]\n",
    "\n",
    "        for aa in range(rows):\n",
    "            if lexical_counts[aa][bb] == 1:\n",
    "                lexical_counts[aa][bb] = 0\n",
    "                unk_count = unk_count + 1\n",
    "\n",
    "        lexical_counts[rows-1][bb] = unk_count\n",
    "    \n",
    "#     Smoothing here\n",
    "    k = 0.01\n",
    "    lexical_counts = np.add(lexical_counts, k)\n",
    "\n",
    "    lexical_probs = np.zeros((rows, cols))\n",
    "\n",
    "    for zz in range(len(lexical_counts)):\n",
    "\n",
    "        lexical_probs[zz] = np.divide(lexical_counts[zz], np.sum(lexical_counts[zz]))\n",
    "\n",
    "    \n",
    "    return lexical_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of counts for training dataset for transition probability\n",
    "transition_tr_data_counts = transition_counts(data)\n",
    "\n",
    "# the transition probabity using K-smoothing method on transition counts\n",
    "transition_prob_chart = transition_prob(transition_tr_data_counts, 0.01)\n",
    "\n",
    "# from training data to get the probability of having each tag as the starting tag\n",
    "start_tag_prob = get_start_probability(data)\n",
    "\n",
    "# emission probability by having a 2D matrix n*m\n",
    "\n",
    "emission_prob_smoothed = emission_2D_prob(data)\n",
    "\n",
    "STATES = ('O','B-LOC','B-ORG','B-PER','B-MISC','I-LOC','I-ORG','I-PER','I-MISC')\n",
    "\n",
    "for key in STATES:\n",
    "    if key in start_tag_prob:\n",
    "        start_tag_prob[ner_dict[key]] = start_tag_prob[key]\n",
    "\n",
    "\n",
    "# Need to get the unk prob for the emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(s_prob, t_prob, e_prob, states, obs):\n",
    "    \n",
    "    #best_prob_state_ind maintains the best probability of getting to a given index ending on a given state\n",
    "    #returns the prob of [index][state]\n",
    "    best_prob_state_ind = dict()\n",
    "    for i in range(len(obs)):\n",
    "        best_prob_state_ind[i] = dict()\n",
    "        for state in states:\n",
    "            state = ner_dict[state]\n",
    "            best_prob_state_ind[i][state] = 1\n",
    "    \n",
    "    #best_path_state_ind maintains the best path of getting to a given index ending on a given state\n",
    "    #returns the path of [state][index]\n",
    "    best_path_state_ind = dict()\n",
    "    for state in states:\n",
    "        state = ner_dict[state]\n",
    "        best_path_state_ind[state] = dict()\n",
    "    \n",
    "    #initializes path probabilities using start_probabilities\n",
    "    for state in states:\n",
    "        state = ner_dict[state]\n",
    "\n",
    "#         print(s_prob[state])\n",
    "#         print(e_prob[word_dict[obs[0]]][state])\n",
    "        \n",
    "#         what does e_prob[state][obs[0]] do?\n",
    "        if state in s_prob:\n",
    "            best_prob_state_ind[0][state] = s_prob[state] * e_prob[word_dict[obs[0]]][state]\n",
    "        else:\n",
    "            best_prob_state_ind[0][state] = 1e-10 * e_prob[word_dict[obs[0]]][state]\n",
    "        best_path_state_ind[state][0] = [state]\n",
    "    \n",
    "    #iterates through index,state and fills both dictionaries\n",
    "    for i in range(1, len(obs)):\n",
    "        for state in states:\n",
    "            state = ner_dict[state]\n",
    "            best_prob = 0\n",
    "            best_path = []\n",
    "            \n",
    "            #iterates through possible previous states to find best probability\n",
    "            for prev_state in states:\n",
    "                prev_state = ner_dict[prev_state]\n",
    "                prev_prob = best_prob_state_ind[i - 1][prev_state]\n",
    "                transition = t_prob[prev_state][state]\n",
    "                if obs[i] in word_dict:\n",
    "                    emission = e_prob[word_dict[obs[i]]][state]\n",
    "                else:\n",
    "                    emission = e_prob[word_dict['UNK'][state]]                     \n",
    "                #probability defined\n",
    "                if(prev_prob * transition * emission > best_prob):\n",
    "                    best_prob = prev_prob * transition * emission\n",
    "                    if i-1 in best_path_state_ind[prev_state]:\n",
    "                        best_path = best_path_state_ind[prev_state][i - 1].append(state)\n",
    "\n",
    "            #update dicts\n",
    "            best_prob_state_ind[i][state] = best_prob\n",
    "            best_path_state_ind[prev_state][i] = best_path\n",
    "    \n",
    "    #find best path\n",
    "    for state in states:\n",
    "        best_prob = 0\n",
    "        best_path = []\n",
    "        \n",
    "        if best_prob_ind[len(obs)][state] > best_prob:\n",
    "            best_path = best_path_state_ind[state][len(obs)]\n",
    "    \n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bd3cf4eba517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mviterbi_tab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_tag_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransition_prob_chart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memission_prob_smoothed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-4f034d772186>\u001b[0m in \u001b[0;36mviterbi\u001b[0;34m(s_prob, t_prob, e_prob, states, obs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmin_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#best_prob_state_ind maintains the best probability of getting to a given index ending on a given state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "viterbi_tab = viterbi(start_tag_prob,transition_prob_chart,emission_prob_smoothed, STATES, test.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
